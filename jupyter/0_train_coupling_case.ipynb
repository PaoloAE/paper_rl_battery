{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dicke quantum battery, coupling scheme: train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter Notebook trains an RL agent to discover the optimal time-dependent coupling control $\\lambda_\\text{c}(t)$ that maximizes the single battery unit ergotropy of the Dicke battery (see [arXiv.2212.12397](https://doi.org/10.1088/1367-2630/8/5/083)). This is the so called \"coupling case\" in the manuscript. \n",
    "\n",
    "Let us consider a setup with $N$ two level systems (TLS) in a cavity. Setting $\\hbar=1$, the Hamiltonian governing the evolution of the total quantum system (charger + battery) is given by\n",
    "\\begin{equation*}\n",
    "\t\\mathcal{\\hat H}(t) = \\mathcal{\\hat H}_{\\rm C}+\\mathcal{\\hat H}_{\\rm B}+\\lambda_\\text{c}(t)\\,\\mathcal{\\hat H}_{\\rm int}~,\n",
    "\\end{equation*}\n",
    "where\n",
    "\\begin{equation*}\n",
    "\t\\mathcal{\\hat H}_{\\rm C} =\\omega_0\\hat{a}^\\dagger\\hat{a}\n",
    "\\end{equation*}\n",
    "is the charger Hamiltonian represented by a single mode cavity, $\\hat{a}^\\dagger,\\hat{a}$ being the bosonic ladder operators.\n",
    "\\begin{align*}\n",
    "\t \\mathcal{\\hat H}_{\\rm B} &= \\sum_{j=1}^N \\hat{h}_j^{\\rm B}~, & \\hat{h}_j^{\\rm B}&=\\frac{\\omega_0}{2}\\big(\\hat{\\sigma}^{(z)}_j+1\\big),\n",
    "\\end{align*}\n",
    "where $\\hat{\\sigma}^{(\\alpha)}_j$ are the $\\alpha=x,y,z$ Pauli matrices acting on the $j$-th TLS, is the battery Hamiltonian, and\n",
    "\\begin{equation*}\n",
    "\t\\mathcal{\\hat H}_{\\rm int}=\\omega_0\\sum_{j=1}^N\\hat{\\sigma}^{(x)}_j (\\hat{a}+\\hat{a}^\\dagger)\n",
    "\\end{equation*}\n",
    "is the interaction Hamiltonian. The total quantum system is initialized in the state\n",
    "\\begin{equation*}\n",
    "\t\\ket{\\Psi_0}=\\ket{{\\rm G}}\\otimes\\ket{N},\n",
    "\\end{equation*}\n",
    "where $\\ket{N}$ is the cavity's Fock state with $N$ excitations, and where\n",
    "\\begin{equation*}\n",
    "\t\\ket{{\\rm G}} = \\otimes_{j=1}^N\\ket{0}_j,\n",
    "\\end{equation*}\n",
    "$\\ket{0}_j$ being the ground state of the $j$-th TLS.\n",
    "\n",
    "Given a charging time $\\tau$, the aim of the RL algorithm is to maximize the single battery unit ergotropy $\\mathcal{E}^{(N)}_1(\\tau)$ at the final time $\\tau$, where\n",
    "\\begin{equation}\n",
    "\t\\mathcal{E}^{(N)}_1(\\tau)=\\frac{\\braket{\\psi(\\tau) |\\mathcal{\\hat H}_{\\rm B}| \\psi(\\tau)}}{N}-r_{1}(\\tau)\\omega_0~,\n",
    "\\end{equation}\n",
    "$r_{1}(\\tau)$ being the minimum eigenvalue of the single TLS reduced density matrix ${\\rho}_{{\\rm B},1}(\\tau)$. Details on the calculation of the ergotropy are given in the appendix of the manuscript.\n",
    "\n",
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"..\", \"src\"))\n",
    "import sac_epi_envs\n",
    "import sac_epi\n",
    "import extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup new Training\n",
    "The following codes initiates a new training session for a given value of $N$ and $\\tau$. All training logs, parameters, policy, and saved states will be stored under the ```data``` folder, within a folder with the current date and time. \n",
    "- ```env_params``` is a dictionary with the environment parameters.\n",
    "- ```training_hyperparams``` is a dictionary with training hyperparameters.\n",
    "- ```log_info``` is a dictionary that specifices which quantities to log.\n",
    "\n",
    "The parameters below were used to produce the results in the manuscript relative to the coupling scheme. Notice that, to reproduce every point in Fig. 1 of the manuscript, the values of ```nq```, ```tau``` and ```dt``` must be varied accordingly (see Manuscript for details). \n",
    "\n",
    "The choice of ```nq```, ```tau``` and ```dt``` below reproduces the furthest green dot in Fig. 1(b), i.e. it is relative to $N=16$ with the largest value of $\\tilde{g}\\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nq = 16                           #number of qubit (two level systems)\n",
    "tau = 5.6                         #charging time tau \n",
    "dt = 0.2                          #duration of a timestep\n",
    "\n",
    "env_params = {\n",
    "    \"wc\": 1.,                     #frequency of the cavity     \n",
    "    \"nc\": nq*2,                   #cutoff value of the Fock space (maximum number of photons in the cavity)\n",
    "    \"nc_i\": nq,                   #number of photons in the cavity at t=0\n",
    "    \"wq\": 1.,                     #frequency of the qubits (two level systems)\n",
    "    \"nq\": nq,                     #number of qubits (two level systems)\n",
    "    \"min_u\": -0.3,                #minimum value of the coupling \\lambda_{\\rm c}(t). This determines \\tilde{g}  \n",
    "    \"max_u\": +0.3,                #maximum value of the coupling \\lambda_{\\rm c}(t). This determines \\tilde{g}\n",
    "    \"dt\": dt,                     #duration of a timestep\n",
    "    \"tau\": tau,                   #charging time\n",
    "    \"reward_coeff\": 1.,           #coefficient multiplying the rewards\n",
    "    \"quantity\": \"ergotropy\"       #quantity whose difference is returned as reward\n",
    "} \n",
    "training_hyperparams = {\n",
    "    \"BATCH_SIZE\": 256,            #batch size\n",
    "    \"LR\": 0.001,                  #learning rate for Q and Pi loss\n",
    "    \"ALPHA_LR\": 0.003,            #learning rate to tune the temperature parameters alpha \n",
    "    \"H_START\": 0.72,              #initial target entropy of the policy\n",
    "    \"H_END\": -3.,                 #final target entropy of the policy\n",
    "    \"H_DECAY\": 200000,            #exponential decay of the target entropy of the policy\n",
    "    \"C_START\": [1./nq, 0.],       #initial weights for energy and ergotropy to compute the reward (Eqs. S38 - S39)\n",
    "    \"C_END\": [0., 1.],            #final weights for energy and ergotropy to compute the reward (Eqs. S38 - S39)\n",
    "    \"C_MEAN\": 40000,              #timestep number where the weights are half way between start and end\n",
    "    \"C_WIDTH\": 20000,             #width in timesteps to transition from start and end weights        \n",
    "    \"REPLAY_MEMORY_SIZE\": 180000, #size of the replay buffer\n",
    "    \"POLYAK\": 0.995,              #polyak coefficient\n",
    "    \"LOG_STEPS\": 1000,            #save logs and display training every number of steps\n",
    "    \"GAMMA\": 0.993,               #RL discount factor\n",
    "    \"RETURN_GAMMA\": 0.9,          #exponential averaging of the return when logging during training\n",
    "    \"LOSS_GAMMA\": 0.995,          #exponential averaging of the loss functions when logging during training\n",
    "    \"HIDDEN_SIZES\": (512,256),    #size of hidden layers of the neural networks\n",
    "    \"SAVE_STATE_STEPS\": 480000,   #saves complete state of trainig every number of steps\n",
    "    \"INITIAL_RANDOM_STEPS\": 5000, #number of initial uniformly random steps\n",
    "    \"UPDATE_AFTER\": 1000,         #start minimizing loss function after initial steps\n",
    "    \"UPDATE_EVERY\": 50,           #performs this many updates every this many steps\n",
    "    \"USE_CUDA\": True,             #use cuda for computation\n",
    "    \"MIN_COV_EIGEN\": 1.e-7,       #security parameter for the covariance matrix of the policy\n",
    "    \"DONT_SAVE_MEMORY\": True      #if true, it won't save the memory buffer, so cannot resume training from file\n",
    "}\n",
    "log_info = {\n",
    "    \"log_running_reward\": True,   #log running reward \n",
    "    \"log_running_loss\": True,     #log running loss\n",
    "    \"log_actions\": True,          #log chosen actions\n",
    "    \"extra_str\": f\"_nq={nq}_tau={np.round(tau,3)}_dt={np.round(dt,3)}\" #string to append to training folder name\n",
    "}\n",
    "\n",
    "#initialize training object\n",
    "train = sac_epi.SacTrain()\n",
    "train.initialize_new_train(sac_epi_envs.DickeBatteryOneControlCoupling, env_params, training_hyperparams, log_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Perform a given number of training steps. It can be run multiple times. While training, the following running averages are plotted:\n",
    "- G: running average of the return, which is a running weighed average of the final energy and ergotropy (see Eqs. S38 S39 of the Manuscript);\n",
    "- Obj 0: the first objective, i.e. the total energy of the battery;\n",
    "- Obj 1: the second objective, i.e. the single TLS ergotropy;\n",
    "- Q Runninng Loss;\n",
    "- Pi Running Loss;\n",
    "- alpha: th temperature parameter of the SAC method;\n",
    "- entropy: the average entropy of the policy;\n",
    "- u: The last 400 value of the time-dependent control that were proposed by the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(480000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear object\n",
    "If the previous block of code is to be run within a loop, it can help to clear the memory running the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra.clear_memory(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the State\n",
    "The full state of the training session is saved every ```SAVE_STATE_STEPS``` steps. Run this command if you wish to manually save the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.save_full_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Training\n",
    "Any training session that was saved can be loaded specifying the training session folder in ```log_dir```. \n",
    "\n",
    "If ```DONT_SAVE_MEMORY: False``` in the hyperparameters, one can set ```no_train=False``` below, and:\n",
    "- this will produce a new folder for logging with the current date-time;\n",
    "- it is then possible to train the model for a longer time.\n",
    "\n",
    "If ```DONT_SAVE_MEMORY: True``` in the hyperparameters, one must set ```no_train=True``` below. This doesn't create a new folder, and doesn't allow to keep training, but one can use this to evaluate the current policy. See the ```2_evaluate_and_export_performance.ipynb``` for this use case.\n",
    "\n",
    "Saving the memory can be useful to keep training loading an older session, but it uses up a lot of space on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"../data/2024_08_15-11_44_05_nq=16_tau=5.6_dt=0.2\" #example of a training folder\n",
    "\n",
    "#create a new SacTrain object\n",
    "train = sac_epi.SacTrain()\n",
    "\n",
    "#load state from a folder\n",
    "train.load_train(log_dir, no_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train\n",
    "If ```no_train=False``` and the memory buffer was saved, one can keep training using the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.train(2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
